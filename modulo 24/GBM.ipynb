{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9da47e96",
   "metadata": {},
   "source": [
    "## 1. Cite 5 diferenças entre o AdaBoost e o GBM.\n",
    "\n",
    "1. AdaBoost usa floresta de stumps enquanto GBM usa arvores se limite de tamanho\n",
    "2. AdaBoost inicia com um stump o GBM inicia com a media da variável resposta\n",
    "3. Cada resposta no AdaBoost tem um peso diferente, no GBM cada resposta possui </br>um multiplicador comum learning_rate(eta)\n",
    "4. GBM utiliza-se dos residuos para calcular as resposta enquanto AdaBoost utiliza</br> faixas de pesos\n",
    "5. AdaBoost usa votação ponderada para fazer a prdição o GBM usa redução no residuo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5ee5b6",
   "metadata": {},
   "source": [
    "## 2. Acesse o link [Scikit-learn – GBM](https://scikit-learn.org/stable/modules/ensemble.html), leia a explicação</br>(traduza se for preciso) e crie um jupyter notebook </br>contendo o exemplo de classificação e de regressão do GBM.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c789d403",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8965"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.datasets import make_hastie_10_2\n",
    "\n",
    "X, y = make_hastie_10_2(random_state=0)\n",
    "X_train, X_test = X[:2000], X[2000:]\n",
    "y_train, y_test = y[:2000], y[2000:]\n",
    "\n",
    "clf = HistGradientBoostingClassifier(max_iter=100).fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "67ceddf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.009154859960321"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.datasets import make_friedman1\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "X, y = make_friedman1(n_samples=1200, random_state=0, noise=1.0)\n",
    "X_train, X_test = X[:200], X[200:]\n",
    "y_train, y_test = y[:200], y[200:]\n",
    "est = GradientBoostingRegressor(\n",
    "n_estimators=100, learning_rate=0.1, max_depth=1, random_state=0,\n",
    "loss='squared_error').fit(X_train, y_train)\n",
    "mean_squared_error(y_test, est.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78106fd2",
   "metadata": {},
   "source": [
    "## 3. Cite 5 Hyperparametros importantes no GBM.\n",
    "1. max_iter\n",
    "2. max_features\n",
    "3. learning_rate\n",
    "4. random_state\n",
    "5. loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66be05ee",
   "metadata": {},
   "source": [
    "## 4. (Opcional) Utilize o GridSearch para encontrar os</br> melhores hyperparametros para o conjunto de dados do exemplo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1b43888a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score:  0.9667496443812233\n",
      "Best params:  {'learning_rate': 0.1, 'loss': 'log_loss', 'max_iter': 10, 'random_state': 1993}\n",
      "CPU times: total: 5.55 s\n",
      "Wall time: 5.89 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "parametros = {'max_iter':[10,200,10],\n",
    "           'learning_rate':[0.05, 0.07, 0.1],\n",
    "           'random_state':[1993],\n",
    "           'loss':['log_loss', 'exponential']\n",
    "}\n",
    "\n",
    "X, y = load_iris(return_X_y=True)\n",
    "\n",
    "clf = HistGradientBoostingClassifier()\n",
    "grid = GridSearchCV(estimator = clf, param_grid = parametros)\n",
    "grid.fit(X, y)\n",
    "\n",
    "scores = cross_val_score(clf, X, y, cv=4)\n",
    "\n",
    "print('Score: ',scores.mean());\n",
    "print('Best params: ', grid.best_params_);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13dddae1",
   "metadata": {},
   "source": [
    "## 5. Acessando o artigo do Jerome Friedman [(Stochastic)](https://jerryfriedman.su.domains/ftp/stobst.pdf)</br> e pensando no nome dado ao Stochastic GBM,</br> qual é a maior diferença entre os dois algoritmos?\n",
    "\n",
    "<blockquote>\n",
    "O Stochastic Gradient Boosting Machine (Stochastic GBM) é um algoritmo que combina Gradient Boosting e Bootstrap Aggregating, sendo um híbrido de Bagging e Boosting. Ele incorpora variáveis aleatórias ao treinar o classificador base em subconjuntos não repetitivos dos dados de treinamento, usando em média metade da amostra em cada iteração. Essa aleatoriedade na amostragem melhora a precisão e a robustez do modelo, reduzindo o overfitting e promovendo uma melhor generalização.</blockquote>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
